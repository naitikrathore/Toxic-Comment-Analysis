{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <script>\n",
       "            function code_toggle_15238811745133085282() {\n",
       "                $('div.cell.code_cell.rendered.selected').find('div.input').toggle();\n",
       "            }\n",
       "\n",
       "            \n",
       "        </script>\n",
       "\n",
       "        <a href=\"javascript:code_toggle_15238811745133085282()\">Toggle show/hide</a>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import HTML\n",
    "import random\n",
    "\n",
    "def hide_toggle(for_next=False):\n",
    "    this_cell = \"\"\"$('div.cell.code_cell.rendered.selected')\"\"\"\n",
    "    next_cell = this_cell + '.next()'\n",
    "\n",
    "    toggle_text = 'Toggle show/hide'  # text shown on toggle link\n",
    "    target_cell = this_cell  # target cell to control with toggle\n",
    "    js_hide_current = ''  # bit of JS to permanently hide code in current cell (only when toggling next cell)\n",
    "\n",
    "    if for_next:\n",
    "        target_cell = next_cell\n",
    "        toggle_text += ' next cell'\n",
    "        js_hide_current = this_cell + '.find(\"div.input\").hide();'\n",
    "\n",
    "    js_f_name = 'code_toggle_{}'.format(str(random.randint(1,2**64)))\n",
    "\n",
    "    html = \"\"\"\n",
    "        <script>\n",
    "            function {f_name}() {{\n",
    "                {cell_selector}.find('div.input').toggle();\n",
    "            }}\n",
    "\n",
    "            {js_hide_current}\n",
    "        </script>\n",
    "\n",
    "        <a href=\"javascript:{f_name}()\">{toggle_text}</a>\n",
    "    \"\"\".format(\n",
    "        f_name=js_f_name,\n",
    "        cell_selector=target_cell,\n",
    "        js_hide_current=js_hide_current, \n",
    "        toggle_text=toggle_text\n",
    "    )\n",
    "\n",
    "    return HTML(html)\n",
    "\n",
    "hide_toggle()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import re\n",
    "import string\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.corpus import stopwords\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "tqdm.pandas()\n",
    "# import nltk\n",
    "# nltk.download(\"stopwords\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>comment_text</th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0000997932d777bf</td>\n",
       "      <td>Explanation\\nWhy the edits made under my usern...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000103f0d9cfb60f</td>\n",
       "      <td>D'aww! He matches this background colour I'm s...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>000113f07ec002fd</td>\n",
       "      <td>Hey man, I'm really not trying to edit war. It...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0001b41b1c6bb37e</td>\n",
       "      <td>\"\\nMore\\nI can't make any real suggestions on ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0001d958c54c6e35</td>\n",
       "      <td>You, sir, are my hero. Any chance you remember...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 id                                       comment_text  toxic  \\\n",
       "0  0000997932d777bf  Explanation\\nWhy the edits made under my usern...      0   \n",
       "1  000103f0d9cfb60f  D'aww! He matches this background colour I'm s...      0   \n",
       "2  000113f07ec002fd  Hey man, I'm really not trying to edit war. It...      0   \n",
       "3  0001b41b1c6bb37e  \"\\nMore\\nI can't make any real suggestions on ...      0   \n",
       "4  0001d958c54c6e35  You, sir, are my hero. Any chance you remember...      0   \n",
       "\n",
       "   severe_toxic  obscene  threat  insult  identity_hate  \n",
       "0             0        0       0       0              0  \n",
       "1             0        0       0       0              0  \n",
       "2             0        0       0       0              0  \n",
       "3             0        0       0       0              0  \n",
       "4             0        0       0       0              0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data = pd.read_csv(\"data/train.csv\")\n",
    "display(data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's understand the given data a bit more"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows in the dataset : 159571\n",
      "Number of rows in the dataset with one or more NA values: 0\n"
     ]
    }
   ],
   "source": [
    "n = data.shape[0]\n",
    "print(\"Number of rows in the dataset : {}\".format(n))\n",
    "n_null = data[data.isna().any(axis = 1)].shape[0]\n",
    "print(\"Number of rows in the dataset with one or more NA values: {}\".format(n_null))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### What's the distribution of comments for each of the 6 categories?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "labels = [\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]\n",
    "category_counts = data[labels].sum(axis = 0)\n",
    "category_counts_df = pd.DataFrame({\"Label\" : category_counts.index, \"Count\" : category_counts.values})\n",
    "ax = sns.barplot(data = category_counts_df, x = \"Label\", y = \"Count\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Do we have messages with more than one label marked ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Number of Comments which do not have any label marked : 143346 (89.83%)'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYsAAAEGCAYAAACUzrmNAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/MnkTPAAAACXBIWXMAAAsTAAALEwEAmpwYAAAYN0lEQVR4nO3dfbRddX3n8feHoIAoCovAMAk1aKMW6AgSKUqrKK2kq47BVdFYldDFNKsUGZ+qA3YtO04ns5xxpmNpBxwGlVAfmIiyACsqjfJQi8QbREN4kBQUUiiJdjmCs0QJ3/lj/+54vJzcfSP33JOH92uts84+37N/+3z3ucn93L33OXunqpAkaTp7jbsBSdLOz7CQJPUyLCRJvQwLSVIvw0KS1GvvcTcwKgcffHAtWrRo3G1I0i5l/fr136uq+VPru21YLFq0iImJiXG3IUm7lCTfHVZ3N5QkqZdhIUnqZVhIknoZFpKkXoaFJKmXYSFJ6mVYSJJ6GRaSpF6GhSSp1277De5hjnv3peNuYYet/+Dp425BktyykCT1MywkSb0MC0lSL8NCktTLsJAk9TIsJEm9DAtJUq+RhkWSZyW5PMmdSe5I8pIkByW5Nsnd7f7AgfnPS7IpyV1JThmoH5dkQ3vu/CQZZd+SpJ836i2LvwC+UFUvAF4I3AGcC6ytqsXA2vaYJEcCy4GjgKXABUnmteVcCKwEFrfb0hH3LUkaMLKwSHIA8DLgIwBV9ZOq+gGwDFjdZlsNnNqmlwGXVdWjVXUvsAk4PslhwAFVdVNVFXDpwBhJ0hwY5ZbFc4CtwMeSfCPJxUn2Bw6tqgcB2v0hbf4FwP0D4ze32oI2PbX+BElWJplIMrF169bZXRtJ2oONMiz2Bl4EXFhVxwI/ou1y2o5hxyFqmvoTi1UXVdWSqloyf/78He1XkrQdowyLzcDmqrq5Pb6cLjwearuWaPdbBuY/fGD8QuCBVl84pC5JmiMjC4uq+ifg/iTPb6WTgduBq4AVrbYCuLJNXwUsT7JPkiPoDmSva7uqHk5yQvsU1OkDYyRJc2DUpyg/B/hEkqcC9wC/TxdQa5KcCdwHnAZQVRuTrKELlMeAs6tqW1vOWcAlwH7ANe0mSZojIw2LqroVWDLkqZO3M/8qYNWQ+gRw9Kw2J0maMb/BLUnqZVhIknoZFpKkXoaFJKmXYSFJ6mVYSJJ6GRaSpF6GhSSpl2EhSeplWEiSehkWkqRehoUkqZdhIUnqZVhIknoZFpKkXoaFJKmXYSFJ6mVYSJJ6GRaSpF6GhSSpl2EhSeplWEiSehkWkqRehoUkqddIwyLJd5JsSHJrkolWOyjJtUnubvcHDsx/XpJNSe5KcspA/bi2nE1Jzk+SUfYtSfp5c7Fl8YqqOqaqlrTH5wJrq2oxsLY9JsmRwHLgKGApcEGSeW3MhcBKYHG7LZ2DviVJzTh2Qy0DVrfp1cCpA/XLqurRqroX2AQcn+Qw4ICquqmqCrh0YIwkaQ6MOiwK+FKS9UlWttqhVfUgQLs/pNUXAPcPjN3cagva9NT6EyRZmWQiycTWrVtncTUkac+294iXf2JVPZDkEODaJHdOM++w4xA1Tf2JxaqLgIsAlixZMnQeSdKOG+mWRVU90O63AFcAxwMPtV1LtPstbfbNwOEDwxcCD7T6wiF1SdIcGVlYJNk/yTMmp4FXAbcBVwEr2mwrgCvb9FXA8iT7JDmC7kD2urar6uEkJ7RPQZ0+MEaSNAdGuRvqUOCK9inXvYFPVtUXknwdWJPkTOA+4DSAqtqYZA1wO/AYcHZVbWvLOgu4BNgPuKbdJElzZGRhUVX3AC8cUv8+cPJ2xqwCVg2pTwBHz3aPkqSZ8RvckqRehoUkqZdhIUnqZVhIknoZFpKkXoaFJKmXYSFJ6mVYSJJ6GRaSpF6GhSSpl2EhSeplWEiSehkWkqRehoUkqZdhIUnqZVhIknoZFpKkXoaFJKmXYSFJ6mVYSJJ67T3uBqSZOvEvTxx3Czvsq+d8ddwtSLPCLQtJUi/DQpLUa+RhkWRekm8k+Vx7fFCSa5Pc3e4PHJj3vCSbktyV5JSB+nFJNrTnzk+SUfctSfqZudiyeBtwx8Djc4G1VbUYWNsek+RIYDlwFLAUuCDJvDbmQmAlsLjdls5B35KkZqRhkWQh8DvAxQPlZcDqNr0aOHWgfllVPVpV9wKbgOOTHAYcUFU3VVUBlw6MkSTNgVFvWXwIeA/w+EDt0Kp6EKDdH9LqC4D7B+bb3GoL2vTUuiRpjowsLJK8GthSVetnOmRIraapD3vNlUkmkkxs3bp1hi8rSeozyi2LE4HXJPkOcBnwyiQfBx5qu5Zo91va/JuBwwfGLwQeaPWFQ+pPUFUXVdWSqloyf/782VwXSdqjjSwsquq8qlpYVYvoDlx/uareDFwFrGizrQCubNNXAcuT7JPkCLoD2evarqqHk5zQPgV1+sAYSdIcGMc3uD8ArElyJnAfcBpAVW1Msga4HXgMOLuqtrUxZwGXAPsB17SbJGmOzElYVNV1wHVt+vvAyduZbxWwakh9Ajh6dB1KkqbjN7glSb0MC0lSL8NCktTLsJAk9ZpRWCR5woUEhtUkSbunmW5Z/OUMa5Kk3dC0H51N8hLgpcD8JO8ceOoAYN7wUZKk3U3f9yyeCjy9zfeMgfoPgdeNqilJ0s5l2rCoquuB65NcUlXfnaOeJEk7mZl+g3ufJBcBiwbHVNUrR9GUJGnnMtOw+DTwYbqLGG3rmVeStJuZaVg8VlUXjrQTPWn3/YdfHXcLO+yX3rdh3C1ImoGZfnT26iR/lOSwJAdN3kbamSRppzHTLYvJ60+8e6BWwHNmtx1J0s5oRmFRVUeMuhFJ0s5rRmGR5PRh9aq6dHbbkSTtjGa6G+rFA9P70l286BbAsJCkPcBMd0OdM/g4yTOBvx5JR5Kknc4veory/wssns1GJEk7r5kes7ia7tNP0J1A8FeANaNqSpK0c5npMYv/OjD9GPDdqto8gn4kSTuhGe2GaicUvJPuzLMHAj8ZZVOSpJ3LTK+U93pgHXAa8Hrg5iSeolyS9hAz3Q31J8CLq2oLQJL5wN8Cl4+qMUnSzmOmn4baazIomu/3jU2yb5J1Sb6ZZGOS97f6QUmuTXJ3uz9wYMx5STYluSvJKQP145JsaM+dnyQ7sI6SpCdppmHxhSRfTHJGkjOAvwE+3zPmUeCVVfVC4BhgaZITgHOBtVW1GFjbHpPkSGA5cBSwFLggyeSlWy8EVtJ9XHdxe16SNEf6tg5+OcmJVfVu4H8C/wp4IXATcNF0Y6vzSHv4lHYrYBmwutVXA6e26WXAZVX1aFXdC2wCjk9yGHBAVd1UVUX3rfHJMZKkOdC3ZfEh4GGAqvpsVb2zqt5Bt1Xxob6FJ5mX5FZgC3BtVd0MHFpVD7ZlPggc0mZfANw/MHxzqy1o01Prw15vZZKJJBNbt27ta0+SNEN9YbGoqr41tVhVE3SXWJ1WVW2rqmOAhXRbCUdPM/uw4xA1TX3Y611UVUuqasn8+fP72pMkzVBfWOw7zXP7zfRFquoHwHV0xxoearuWaPeTB843A4cPDFsIPNDqC4fUJUlzpC8svp7kD6YWk5wJrJ9uYJL5SZ7VpvcDfpPui31X8bOLKa0ArmzTVwHLk+yT5Ai6A9nr2q6qh5Oc0D4FdfrAGEnSHOj7nsXbgSuSvImfhcMS4KnAa3vGHgasbp9o2gtYU1WfS3ITsKYFzn10X/SjqjYmWQPcTndKkbOraltb1lnAJXRbM9e0myRpjkwbFlX1EPDSJK8AJo83/E1Vfblvwe1Yx7FD6t+nux7GsDGrgFVD6hMDry9JmmMzvZ7FV4CvjLgXSdJO6he9noUkaQ9iWEiSehkWkqRehoUkqZdhIUnqZVhIknoZFpKkXoaFJKmXYSFJ6mVYSJJ6GRaSpF6GhSSpl2EhSeplWEiSehkWkqRehoUkqZdhIUnqZVhIknoZFpKkXoaFJKmXYSFJ6mVYSJJ6GRaSpF4jC4skhyf5SpI7kmxM8rZWPyjJtUnubvcHDow5L8mmJHclOWWgflySDe2585NkVH1Lkp5olFsWjwHvqqpfAU4Azk5yJHAusLaqFgNr22Pac8uBo4ClwAVJ5rVlXQisBBa329IR9i1JmmJkYVFVD1bVLW36YeAOYAGwDFjdZlsNnNqmlwGXVdWjVXUvsAk4PslhwAFVdVNVFXDpwBhJ0hyYk2MWSRYBxwI3A4dW1YPQBQpwSJttAXD/wLDNrbagTU+tD3udlUkmkkxs3bp1VtdBkvZkIw+LJE8HPgO8vap+ON2sQ2o1Tf2JxaqLqmpJVS2ZP3/+jjcrSRpqpGGR5Cl0QfGJqvpsKz/Udi3R7re0+mbg8IHhC4EHWn3hkLokaY6M8tNQAT4C3FFVfz7w1FXAija9ArhyoL48yT5JjqA7kL2u7ap6OMkJbZmnD4yRJM2BvUe47BOBtwAbktzaau8FPgCsSXImcB9wGkBVbUyyBrid7pNUZ1fVtjbuLOASYD/gmnaTJM2RkYVFVf0dw483AJy8nTGrgFVD6hPA0bPXnSRpR/gNbklSr1HuhpK0A65/2cvH3cIOefkN14+7Bc0htywkSb0MC0lSL8NCktTLsJAk9TIsJEm9DAtJUi/DQpLUy7CQJPUyLCRJvQwLSVIvw0KS1MuwkCT1MiwkSb0MC0lSL8NCktTLsJAk9TIsJEm9DAtJUi/DQpLUy7CQJPUyLCRJvUYWFkk+mmRLktsGagcluTbJ3e3+wIHnzkuyKcldSU4ZqB+XZEN77vwkGVXPkqThRrllcQmwdErtXGBtVS0G1rbHJDkSWA4c1cZckGReG3MhsBJY3G5TlylJGrGRhUVV3QD885TyMmB1m14NnDpQv6yqHq2qe4FNwPFJDgMOqKqbqqqASwfGSJLmyFwfszi0qh4EaPeHtPoC4P6B+Ta32oI2PbU+VJKVSSaSTGzdunVWG5ekPdnOcoB72HGImqY+VFVdVFVLqmrJ/PnzZ605SdrTzXVYPNR2LdHut7T6ZuDwgfkWAg+0+sIhdUnSHJrrsLgKWNGmVwBXDtSXJ9knyRF0B7LXtV1VDyc5oX0K6vSBMZKkObL3qBac5FPAScDBSTYDfwp8AFiT5EzgPuA0gKramGQNcDvwGHB2VW1rizqL7pNV+wHXtJskaQ6NLCyq6o3beerk7cy/Clg1pD4BHD2LrUmSdtDOcoBbkrQTMywkSb0MC0lSL8NCktTLsJAk9TIsJEm9DAtJUi/DQpLUy7CQJPUyLCRJvQwLSVIvw0KS1MuwkCT1MiwkSb1GdopySZr0V++6etwt7LC3/rd/Pe4WdipuWUiSehkWkqRehoUkqZdhIUnqZVhIknoZFpKkXoaFJKmXYSFJ6mVYSJJ67TJhkWRpkruSbEpy7rj7kaQ9yS4RFknmAf8D+G3gSOCNSY4cb1eStOfYVc4NdTywqaruAUhyGbAMuH2sXUkSsOrNrxt3CzvsTz5++Q7Nn6oaUSuzJ8nrgKVV9W/a47cAv1ZVb50y30pgZXv4fOCuOWzzYOB7c/h6c2l3Xjdw/XZ1rt/senZVzZ9a3FW2LDKk9oSUq6qLgItG384TJZmoqiXjeO1R253XDVy/XZ3rNzd2iWMWwGbg8IHHC4EHxtSLJO1xdpWw+DqwOMkRSZ4KLAeuGnNPkrTH2CV2Q1XVY0neCnwRmAd8tKo2jrmtqcay+2uO7M7rBq7frs71mwO7xAFuSdJ47Sq7oSRJY2RYSJJ6GRZPUpKPJtmS5LZx9zLbkhye5CtJ7kiyMcnbxt3TbEqyb5J1Sb7Z1u/94+5ptiWZl+QbST437l5mW5LvJNmQ5NYkE+PuZ7YleVaSy5Pc2f4PvmSs/XjM4slJ8jLgEeDSqjp63P3MpiSHAYdV1S1JngGsB06tqt3im/NJAuxfVY8keQrwd8DbquprY25t1iR5J7AEOKCqXj3ufmZTku8AS6pqt/xCXpLVwI1VdXH7FOjTquoH4+rHLYsnqapuAP553H2MQlU9WFW3tOmHgTuABePtavZU55H28Cntttv89ZRkIfA7wMXj7kU7JskBwMuAjwBU1U/GGRRgWGiGkiwCjgVuHnMrs6rtprkV2AJcW1W70/p9CHgP8PiY+xiVAr6UZH071c/u5DnAVuBjbTfixUn2H2dDhoV6JXk68Bng7VX1w3H3M5uqaltVHUN3VoDjk+wWuxKTvBrYUlXrx93LCJ1YVS+iOxv12W2X8O5ib+BFwIVVdSzwI2Csl2YwLDStti//M8Anquqz4+5nVNom/nXA0vF2MmtOBF7T9utfBrwyycfH29LsqqoH2v0W4Aq6s1PvLjYDmwe2dC+nC4+xMSy0Xe0A8EeAO6rqz8fdz2xLMj/Js9r0fsBvAneOtalZUlXnVdXCqlpEd3qcL1fVm8fc1qxJsn/70AVt98yrgN3mE4lV9U/A/Ume30onM+ZLMuwSp/vYmSX5FHAScHCSzcCfVtVHxtvVrDkReAuwoe3XB3hvVX1+fC3NqsOA1e3iWnsBa6pqt/uI6W7qUOCK7u8Z9gY+WVVfGG9Ls+4c4BPtk1D3AL8/zmb86KwkqZe7oSRJvQwLSVIvw0KS1MuwkCT1MiwkSb0MC81Ikn+R5LIk/5Dk9iSfT/K8cfe1PUlOSvLSWV7m37f7RUl+b6B+RpK/ms3X2t2092za70G0n9kOfXQ5yXVJljy57jQThoV6tS/nXQFcV1XPraojgffSfdZ9Z3USMKthUVWTy1sE/N40s+5U2vdIRrVsv6u1hzAsNBOvAH5aVR+eLFTVrVV1YzofTHJbu7bAG+D//5V4fZI1Sb6d5ANJ3tSuH7EhyXPbfJckubBdN+OeJC9v1wi5I8klk6+X5FVJbkpyS5JPt/NVTV7T4P2tviHJC9pJD/8QeEe71sFvJDmt9fjNJDdMXcEkFyR5TZu+IslH2/SZSf5jm548Q+0HgN9oy35Hq/3LJF9IcneS/zLsTUxycjsp3Ia2jvtsbx1aff8239fbuGVDljnd+/+VJJ8ENgwZ90iS/5zuJHx/m+T49lf6PQPvw75JPtaW+40kr2j1M9rP4Gq6E/n19jnltRclubGt7y1TtgAPaO//7Uk+nGSvNmboz19zqKq8eZv2Bvxb4L9v57nfBa4F5tFtadxH983ok4AftOl9gH8E3t/GvA34UJu+hO7cRQGWAT8EfpXuD5n1wDHAwcANdNeeAPh3wPva9HeAc9r0HwEXt+l/D/zxQJ8bgAVt+llD1mM58ME2vQ74Wpv+GHBKm36k3Z8EfG5g7Bl037B9JrAv8F3g8CnL3xe4H3hee3wp3YkZp1uH/wS8ebJn4NuT78EM3/8fAUds5+dWwG+36SuAL9Gdov2FwK2t/i7gY236BW3Z+7b13QwctAN9LgJua9NPA/Zt04uBiYH39cd0Z1yd19brdT0//+vormkx9v8nu/vNLQs9Wb8OfKq6s7c+BFwPvLg99/XqronxKPAPdL+QoPvFvWhgGVdX9z9/A/BQVW2oqseBjW2+E4Ajga+mO+3ICuDZA+MnT3C4fspyB30VuCTJH9D9IprqRrqthSPpzsHzULqLP70E+Pu+NwFYW1X/p6p+3MY/e8rzzwfurapvt8er6a5XMN06vAo4t63zdXS/qH9pynKne//XVdW92+n3J8Dk6TE2ANdX1U/5+Z/NrwN/DVBVd9KF4ORxqmuravI6LjPpc9BTgP+VZAPwabqf7aR1VXVPVW0DPtV66Pv5aw64v1EzsZHuL7xhMs24RwemHx94/Dg//2/v0SHzDM63je6X0xt7Xmcb2/k3XVV/mOTX6C4GdGuSY6rq+wPP/2OSA+nOOnsDcBDwerqtiYenWcepPWyvj+nep+2tQ4Dfraq7phk33XJ/NM1zP20BDQPve1U9np8dh5jpsmfS56B3AA/RbcXsRbc1MWnq+YeqLX+6n7/mgFsWmokvA/u0v8oBSPLiJC+n+8X6hnQXEZpP99fyull+/a8BJyb55fbaT0v/J7EeBp4x0O9zq+rmqnof8D3g8CFjbgLeTrdONwJ/3O6nXfYM3QksmlwHuhM0Xt8z5ovAOUl3trwkxw6ZZ5Tv/w3Am9prP49ua2FYIMykz0HPBB5sW49v4ee39I5PckQ7VvEGukvd/iI/f80yw0K92l+grwV+K91HZzfSHRN4gG5/97eAb9KFynuqO73ybL7+Vrr95J9K8i26Xx4v6Bl2NfDayQPcwAfbgdrb6H4JfnPImBuBvatqE3AL3dbFsLD4FvBYO1j+jiHPD1uHH9OdNfTTbffL48CHpx/Fn9HtsvlW6/vPhswzyvf/AmBe6/d/A2e0XYq/SJ9Tl7siydfodmsNbqXcRPcBgtuAe4ErfsGfv2aZZ52VJPVyy0KS1MuwkCT1MiwkSb0MC0lSL8NCktTLsJAk9TIsJEm9/h+ZCDOroTArNQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "counts = data[labels].sum(axis = 1)\n",
    "counts_df = pd.DataFrame({\"Comments with one or more label\" : counts.value_counts().index, \"Count\" : counts.value_counts().values})\n",
    "no_label_count = counts_df[counts_df[\"Comments with one or more label\"] == 0][\"Count\"].values[0]\n",
    "display(\"Number of Comments which do not have any label marked : {} ({}%)\".format(no_label_count, round(100 * no_label_count/data.shape[0], 2)))\n",
    "ax = sns.barplot(data = counts_df[counts_df[\"Comments with one or more label\"] > 0], x = \"Comments with one or more label\", y = \"Count\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### What about toxic and severe_toxic category ? They seem related."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>severe_toxic</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>toxic</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>144277</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>13699</td>\n",
       "      <td>1595</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "severe_toxic       0     1\n",
       "toxic                     \n",
       "0             144277     0\n",
       "1              13699  1595"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'Comments which are marked as sever_toxic are also marked as toxic. Hence the comments with severe_toxic category is a subset of comments with toxic category.'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(pd.crosstab(data[\"toxic\"], data[\"severe_toxic\"]))\n",
    "display(\"Comments which are marked as sever_toxic are also marked as toxic. Hence the comments with severe_toxic category \"\n",
    "        \"is a subset of comments with toxic category.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### How do the messages look like ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Examples of comments within toxic category - '"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\"Bye! \\n\\nDon't look, come or think of comming back! Tosser.\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'Hi! I am back again!\\nLast warning!\\nStop undoing my edits or die!'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(\"Examples of comments within toxic category - \")\n",
    "display(data[data[\"toxic\"] == 1][\"comment_text\"].values[2])\n",
    "display(data[data[\"toxic\"] == 1][\"comment_text\"].values[12])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Examples of comments within threat category - '"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'Whoever put a notices on my page. I will kill u'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\"WHAT'S WRONG WITH YOU?\\nGO KILL YOURSELF, YOU VILE CREATURE!!!!!!!!!!!\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(\"Examples of comments within threat category - \")\n",
    "display(data[data[\"threat\"] == 1][\"comment_text\"].values[5])\n",
    "display(data[data[\"threat\"] == 1][\"comment_text\"].values[13])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### What are most commonly used words for each of the labels ? \n",
    "**This section can be skipped, as it contains words which might be vulgar, profane or offensive.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_comment(comment, remove_stopwords = True, english_stopwords = None):\n",
    "    \"\"\"\n",
    "    This function would process the comment string. It mainly performs the following actions - \n",
    "    1. Remove all the punctuations from the string.\n",
    "    2. Transform the string to contain only lowercase alphabets. \n",
    "    3. Remove the stop-words from the string. (Optional)\n",
    "    \"\"\"\n",
    "    comment = re.sub(f\"[{string.punctuation}\\s+]\", \" \", comment) # Replace all punctuation marks and whitespace characters with a single space.\n",
    "    comment = comment.lower() # Converts the string to lowercase.\n",
    "    \n",
    "    if(remove_stopwords):\n",
    "        comment = \" \".join(filter(lambda x : x not in english_stopwords, comment.split())) # Removes stopwords from the string\n",
    "    return comment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93mstopwords\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('stopwords')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/stopwords\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\hp/nltk_data'\n    - 'C:\\\\Users\\\\hp\\\\anaconda3\\\\nltk_data'\n    - 'C:\\\\Users\\\\hp\\\\anaconda3\\\\share\\\\nltk_data'\n    - 'C:\\\\Users\\\\hp\\\\anaconda3\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\hp\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\nltk\\corpus\\util.py\u001b[0m in \u001b[0;36m__load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     83\u001b[0m                 \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 84\u001b[1;33m                     \u001b[0mroot\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"{self.subdir}/{zip_name}\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     85\u001b[0m                 \u001b[1;32mexcept\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\nltk\\data.py\u001b[0m in \u001b[0;36mfind\u001b[1;34m(resource_name, paths)\u001b[0m\n\u001b[0;32m    582\u001b[0m     \u001b[0mresource_not_found\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34mf\"\\n{sep}\\n{msg}\\n{sep}\\n\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 583\u001b[1;33m     \u001b[1;32mraise\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresource_not_found\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    584\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mstopwords\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('stopwords')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/stopwords.zip/stopwords/\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\hp/nltk_data'\n    - 'C:\\\\Users\\\\hp\\\\anaconda3\\\\nltk_data'\n    - 'C:\\\\Users\\\\hp\\\\anaconda3\\\\share\\\\nltk_data'\n    - 'C:\\\\Users\\\\hp\\\\anaconda3\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\hp\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_7436/3078167336.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0menglish_stopwords\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstopwords\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwords\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"english\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"processed_comment_text\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"comment_text\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprogress_apply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m \u001b[1;33m:\u001b[0m \u001b[0mpreprocess_comment\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0menglish_stopwords\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\nltk\\corpus\\util.py\u001b[0m in \u001b[0;36m__getattr__\u001b[1;34m(self, attr)\u001b[0m\n\u001b[0;32m    119\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"LazyCorpusLoader object has no attribute '__bases__'\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    120\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 121\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__load\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    122\u001b[0m         \u001b[1;31m# This looks circular, but its not, since __load() changes our\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    123\u001b[0m         \u001b[1;31m# __class__ to something new:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\nltk\\corpus\\util.py\u001b[0m in \u001b[0;36m__load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     84\u001b[0m                     \u001b[0mroot\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"{self.subdir}/{zip_name}\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     85\u001b[0m                 \u001b[1;32mexcept\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 86\u001b[1;33m                     \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     87\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     88\u001b[0m         \u001b[1;31m# Load the corpus.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\nltk\\corpus\\util.py\u001b[0m in \u001b[0;36m__load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     79\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     80\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 81\u001b[1;33m                 \u001b[0mroot\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"{self.subdir}/{self.__name}\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     82\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mLookupError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     83\u001b[0m                 \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\nltk\\data.py\u001b[0m in \u001b[0;36mfind\u001b[1;34m(resource_name, paths)\u001b[0m\n\u001b[0;32m    581\u001b[0m     \u001b[0msep\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"*\"\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;36m70\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    582\u001b[0m     \u001b[0mresource_not_found\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34mf\"\\n{sep}\\n{msg}\\n{sep}\\n\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 583\u001b[1;33m     \u001b[1;32mraise\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresource_not_found\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    584\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    585\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mstopwords\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('stopwords')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/stopwords\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\hp/nltk_data'\n    - 'C:\\\\Users\\\\hp\\\\anaconda3\\\\nltk_data'\n    - 'C:\\\\Users\\\\hp\\\\anaconda3\\\\share\\\\nltk_data'\n    - 'C:\\\\Users\\\\hp\\\\anaconda3\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\hp\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "english_stopwords = stopwords.words(\"english\")\n",
    "data[\"processed_comment_text\"] = data[\"comment_text\"].progress_apply(lambda x : preprocess_comment(x, True, english_stopwords))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(3,2, figsize = (16,16))\n",
    "for label, ax in zip(labels, axes.flatten()):\n",
    "    all_comments = data[data[label] == 1][\"processed_comment_text\"].values\n",
    "    df_word_counts = pd.DataFrame(sorted(dict(Counter(\" \".join(all_comments).split(\" \"))).items(), key = lambda x : x[1], reverse = True)[:10])\n",
    "    df_word_counts.columns = [\"Word\", \"Count\"]\n",
    "    sns.barplot(data=df_word_counts, x = \"Count\", y =\"Word\", ax = ax).set_title(\"Top 10 words used in {} comments\".format(label))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How do we represent these comments as numbers (or vector of numbers) ?\n",
    "\n",
    "Most of the Machine Learning models do not accept text as input. Instead they expect numbers (or vector of numbers) as features. So the question arises - How do we convert comments (strings) into numbers ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bag-of-Words representation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A bag-of-words (BoW) model is a way of extracting features from text, which can be then used by Machine Learning models. Is is quite simple and flexible.\n",
    "\n",
    "To create a BoW representation of a textual input, we mainly conisder the following two things - \n",
    "1. A vocabulary of known words - This can be all the unique words in the given corpus.\n",
    "2. A measure of presence of words - This can be the count of number of times a word is present in the given text.\n",
    "   \n",
    "   \n",
    "Eg - \n",
    "Let the corpus contain the following 3 sentences - \n",
    "    1. It was the best of times\n",
    "    2. It was the worst of times\n",
    "    3. It was the age of wisdom and the age of foolishness\n",
    "    \n",
    "Considering the above 3 sentences the vocabulary consist of following words (punctuation and case is ignored) -\n",
    "    * it\n",
    "    * was\n",
    "    * the\n",
    "    * best\n",
    "    * of\n",
    "    * times\n",
    "    * worst\n",
    "    * age\n",
    "    * wisdom\n",
    "    * and\n",
    "    * foolishness\n",
    "    \n",
    "Once the vocabulary is known, the BoW representation of the text is just the frequencty of each word in the vocabulary for the text. \n",
    "Hence, the BoW representation for the 1st sentence in the corpus is - "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Sentence** | it | was | the | best | of | times | worst | age | wisdom | and | foolishness\n",
    ":--- | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---:\n",
    "**It was the best of times** | 1 | 1 | 1 | 1 | 1 | 1 | 0 | 0 | 0 | 0 | 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly for other sentences in the corpus - "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Sentence** | it | was | the | best | of | times | worst | age | wisdom | and | foolishness\n",
    ":--- | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---:\n",
    "**It was the best of times** | 1 | 1 | 1 | 1 | 1 | 1 | 0 | 0 | 0 | 0 | 0\n",
    "**It was the worst of times** | 1 | 1 | 1 | 0 | 1 | 1 | 1 | 0 | 0 | 0 | 0\n",
    "**It was the age of wisdom and the age of foolishness** | 1 | 1 | 2 | 0 | 2 | 0 | 0 | 2 | 1 | 1 | 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above matrix of BoW representation can now be used to train any ML model. Sklearn has a flexible implemenation of BoW model, you can read more about it [here](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html#sklearn.feature_extraction.text.CountVectorizer).\n",
    "\n",
    "To numerically represnt a text, we usually compute a score for each word to signify its importance in the text and corpus. In BoW, the score is simply the frequency of the word. \n",
    "\n",
    "A problem with BoW model is that the highly frequent words start to dominate in the document (in NLP we generally refere each text input as a document), but it may not contain as much information as the rarer words. One approach to fix this is by scaling the frequency of words by how often they appear in all the documents, hence penalizing the most common words like 'the', 'was', 'it' etc from the above example. This approach is called *Term Frequency - Inverse Document Frequency (TF-IDF)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Term Frequency - Inverse Document Frequency (TF-IDF) representation\n",
    "\n",
    "TF-IDF is yet another way to numerically represent a text by looking at the words present in the text. TF-IDF value for a word in a text increases proportinally to the number of times a word appears in the text and is offset by the number of texts in the corpus that contain the word, this helps adjust for the words that appear more frequently in general.\n",
    "\n",
    "TF-IDF is a product of two statistics - \n",
    "\n",
    "**1. Term Frequency (TF)** - It is simply the count of a word in a given text. As the length of the text can varry, TF is generally divided by the number of words in the text. Therefore, the term-frequency for a term $t$ in a document $d$ can be calculated as - \n",
    "\n",
    "$$tf(t,d) = \\frac{f_{t,d}}{\\sum_{t' \\in d} f_{t',d}} $$\n",
    "\n",
    "Here, \n",
    "* $f_{t,d}$ is the count of term $t$ in document $d$\n",
    "\n",
    "The TF for the above example is - "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>it</th>\n",
       "      <th>was</th>\n",
       "      <th>the</th>\n",
       "      <th>best</th>\n",
       "      <th>of</th>\n",
       "      <th>times</th>\n",
       "      <th>worst</th>\n",
       "      <th>age</th>\n",
       "      <th>wisdom</th>\n",
       "      <th>and</th>\n",
       "      <th>foolishness</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>it was the best of times</th>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>it was the worst of times</th>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>it was the age of wisdom and the age of foolishness</th>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.181818</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.181818</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.181818</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.090909</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                          it       was  \\\n",
       "it was the best of times                            0.166667  0.166667   \n",
       "it was the worst of times                           0.166667  0.166667   \n",
       "it was the age of wisdom and the age of foolish...  0.090909  0.090909   \n",
       "\n",
       "                                                         the      best  \\\n",
       "it was the best of times                            0.166667  0.166667   \n",
       "it was the worst of times                           0.166667  0.000000   \n",
       "it was the age of wisdom and the age of foolish...  0.181818  0.000000   \n",
       "\n",
       "                                                          of     times  \\\n",
       "it was the best of times                            0.166667  0.166667   \n",
       "it was the worst of times                           0.166667  0.166667   \n",
       "it was the age of wisdom and the age of foolish...  0.181818  0.000000   \n",
       "\n",
       "                                                       worst       age  \\\n",
       "it was the best of times                            0.000000  0.000000   \n",
       "it was the worst of times                           0.166667  0.000000   \n",
       "it was the age of wisdom and the age of foolish...  0.000000  0.181818   \n",
       "\n",
       "                                                      wisdom       and  \\\n",
       "it was the best of times                            0.000000  0.000000   \n",
       "it was the worst of times                           0.000000  0.000000   \n",
       "it was the age of wisdom and the age of foolish...  0.090909  0.090909   \n",
       "\n",
       "                                                    foolishness  \n",
       "it was the best of times                               0.000000  \n",
       "it was the worst of times                              0.000000  \n",
       "it was the age of wisdom and the age of foolish...     0.090909  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <script>\n",
       "            function code_toggle_4628852606177926414() {\n",
       "                $('div.cell.code_cell.rendered.selected').find('div.input').toggle();\n",
       "            }\n",
       "\n",
       "            \n",
       "        </script>\n",
       "\n",
       "        <a href=\"javascript:code_toggle_4628852606177926414()\">Toggle show/hide</a>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This cell only contains code written to produce the output table.\n",
    "# This can be hidden (using a toggle button) while executing the notebook\n",
    "def get_tf(documents):\n",
    "    vocabulary = []\n",
    "    for d in documents:\n",
    "        vocabulary.extend(d.split())\n",
    "    vocabulary = list(set(vocabulary))\n",
    "    tf = {}\n",
    "    for d in documents:\n",
    "        tf[d] = {}\n",
    "        for w in vocabulary:\n",
    "            tf[d][w] = d.split().count(w) / len(d.split())\n",
    "    \n",
    "    return pd.DataFrame(tf).T\n",
    "\n",
    "corpus = [ \n",
    "    \"it was the best of times\",\n",
    "    \"it was the worst of times\",\n",
    "    \"it was the age of wisdom and the age of foolishness\"\n",
    "]\n",
    "\n",
    "tf = get_tf(corpus)\n",
    "tf = tf[[\"it\", \"was\", \"the\", \"best\", \"of\", \"times\", \"worst\", \"age\", \"wisdom\", \"and\", \"foolishness\"]]\n",
    "display(tf)\n",
    "\n",
    "hide_toggle()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. Inverse Document Frequency** - It is the measure of how much information the word provides, i.e., if it is a common or rare across all documents. Mathematically, it is the logarithmically scaled inverse fraction of the documents that contain the word. For rare words, IDF value naturally becomes higher and for common words it becomes smaller. The log factor hels dampen the IDF value for large corpuses. The Inverse Document Frequency of a term $t$ in a cospus set $D$ can be caluclated as - \n",
    "$$idf(t,D) = log(\\frac{N}{|{d \\in D : t \\in d}|})$$\n",
    "\n",
    "Here,\n",
    "* $N$ is the total number of documents in the corpus.\n",
    "* $|{d \\in D : t \\in d}|$ is the number of documents where word $t$ occurs.\n",
    "\n",
    "The IDF for the above mentioned example is - "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>it</th>\n",
       "      <th>was</th>\n",
       "      <th>the</th>\n",
       "      <th>best</th>\n",
       "      <th>of</th>\n",
       "      <th>times</th>\n",
       "      <th>worst</th>\n",
       "      <th>age</th>\n",
       "      <th>wisdom</th>\n",
       "      <th>and</th>\n",
       "      <th>foolishness</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>it was the best of times</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.098612</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.405465</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>it was the worst of times</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.405465</td>\n",
       "      <td>1.098612</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>it was the age of wisdom and the age of foolishness</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.098612</td>\n",
       "      <td>1.098612</td>\n",
       "      <td>1.098612</td>\n",
       "      <td>1.098612</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     it  was  the      best  \\\n",
       "it was the best of times                            0.0  0.0  0.0  1.098612   \n",
       "it was the worst of times                           0.0  0.0  0.0  0.000000   \n",
       "it was the age of wisdom and the age of foolish...  0.0  0.0  0.0  0.000000   \n",
       "\n",
       "                                                     of     times     worst  \\\n",
       "it was the best of times                            0.0  0.405465  0.000000   \n",
       "it was the worst of times                           0.0  0.405465  1.098612   \n",
       "it was the age of wisdom and the age of foolish...  0.0  0.000000  0.000000   \n",
       "\n",
       "                                                         age    wisdom  \\\n",
       "it was the best of times                            0.000000  0.000000   \n",
       "it was the worst of times                           0.000000  0.000000   \n",
       "it was the age of wisdom and the age of foolish...  1.098612  1.098612   \n",
       "\n",
       "                                                         and  foolishness  \n",
       "it was the best of times                            0.000000     0.000000  \n",
       "it was the worst of times                           0.000000     0.000000  \n",
       "it was the age of wisdom and the age of foolish...  1.098612     1.098612  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <script>\n",
       "            function code_toggle_18008450337248338751() {\n",
       "                $('div.cell.code_cell.rendered.selected').find('div.input').toggle();\n",
       "            }\n",
       "\n",
       "            \n",
       "        </script>\n",
       "\n",
       "        <a href=\"javascript:code_toggle_18008450337248338751()\">Toggle show/hide</a>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This cell only contains code written to produce the output table.\n",
    "# This can be hidden (using a toggle button) while executing the notebook\n",
    "def get_idf(documents):\n",
    "    vocabulary = []\n",
    "    for d in documents:\n",
    "        vocabulary.extend(d.split())\n",
    "    vocabulary = list(set(vocabulary))\n",
    "    \n",
    "    df = {}\n",
    "    for w in vocabulary:\n",
    "        df[w] = 0\n",
    "        for d in documents:\n",
    "            if w in d:\n",
    "                df[w] = df[w] + 1\n",
    "    idf_words = {}\n",
    "    for k,v in df.items():\n",
    "        idf_words[k] = np.log(len(documents)/v)\n",
    "    \n",
    "    idf = {}\n",
    "    for d in documents:\n",
    "        idf[d] = {}\n",
    "        for w in vocabulary:\n",
    "            if w in d:\n",
    "                idf[d][w] = idf_words[w]\n",
    "            else:\n",
    "                idf[d][w] = 0\n",
    "    return pd.DataFrame(idf).T\n",
    "\n",
    "corpus = [ \n",
    "    \"it was the best of times\",\n",
    "    \"it was the worst of times\",\n",
    "    \"it was the age of wisdom and the age of foolishness\"\n",
    "]\n",
    "\n",
    "idf = get_idf(corpus)\n",
    "idf = idf[[\"it\", \"was\", \"the\", \"best\", \"of\", \"times\", \"worst\", \"age\", \"wisdom\", \"and\", \"foolishness\"]]\n",
    "display(idf)\n",
    "\n",
    "hide_toggle()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally TF-IDF for a term $t$ in a document $d$ within a corpus set $D$ can be calculated as - \n",
    "\n",
    "$$tfidf(t,d,D) = tf(t,d).idf(t,D)$$\n",
    "\n",
    "TF-IDF calculated for the above example is - "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>it</th>\n",
       "      <th>was</th>\n",
       "      <th>the</th>\n",
       "      <th>best</th>\n",
       "      <th>of</th>\n",
       "      <th>times</th>\n",
       "      <th>worst</th>\n",
       "      <th>age</th>\n",
       "      <th>wisdom</th>\n",
       "      <th>and</th>\n",
       "      <th>foolishness</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>it was the best of times</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.183102</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.067578</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>it was the worst of times</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.067578</td>\n",
       "      <td>0.183102</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>it was the age of wisdom and the age of foolishness</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.199748</td>\n",
       "      <td>0.099874</td>\n",
       "      <td>0.099874</td>\n",
       "      <td>0.099874</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     it  was  the      best  \\\n",
       "it was the best of times                            0.0  0.0  0.0  0.183102   \n",
       "it was the worst of times                           0.0  0.0  0.0  0.000000   \n",
       "it was the age of wisdom and the age of foolish...  0.0  0.0  0.0  0.000000   \n",
       "\n",
       "                                                     of     times     worst  \\\n",
       "it was the best of times                            0.0  0.067578  0.000000   \n",
       "it was the worst of times                           0.0  0.067578  0.183102   \n",
       "it was the age of wisdom and the age of foolish...  0.0  0.000000  0.000000   \n",
       "\n",
       "                                                         age    wisdom  \\\n",
       "it was the best of times                            0.000000  0.000000   \n",
       "it was the worst of times                           0.000000  0.000000   \n",
       "it was the age of wisdom and the age of foolish...  0.199748  0.099874   \n",
       "\n",
       "                                                         and  foolishness  \n",
       "it was the best of times                            0.000000     0.000000  \n",
       "it was the worst of times                           0.000000     0.000000  \n",
       "it was the age of wisdom and the age of foolish...  0.099874     0.099874  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <script>\n",
       "            function code_toggle_1314290772282343093() {\n",
       "                $('div.cell.code_cell.rendered.selected').find('div.input').toggle();\n",
       "            }\n",
       "\n",
       "            \n",
       "        </script>\n",
       "\n",
       "        <a href=\"javascript:code_toggle_1314290772282343093()\">Toggle show/hide</a>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This cell only contains code written to produce the output table.\n",
    "# This can be hidden (using a toggle button) while executing the notebook\n",
    "corpus = [ \n",
    "    \"it was the best of times\",\n",
    "    \"it was the worst of times\",\n",
    "    \"it was the age of wisdom and the age of foolishness\"\n",
    "]\n",
    "\n",
    "tf = get_tf(corpus)\n",
    "tf = tf[[\"it\", \"was\", \"the\", \"best\", \"of\", \"times\", \"worst\", \"age\", \"wisdom\", \"and\", \"foolishness\"]]\n",
    "\n",
    "idf = get_idf(corpus)\n",
    "idf = idf[[\"it\", \"was\", \"the\", \"best\", \"of\", \"times\", \"worst\", \"age\", \"wisdom\", \"and\", \"foolishness\"]]\n",
    "\n",
    "display(tf*idf)\n",
    "\n",
    "hide_toggle()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sklearn has a flexible implemenation of TF-IDF, you can read more about it [here](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How should we finally train the model ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Going to initial problem at hand again - we need to identify the type of toxicity (toxic, obscene, threat etc) for a comment. A comment can have more than one type of toxicity. If can also have none.\n",
    "\n",
    "If we look at each of the 6 toxicity labels individually, then the problem can be restructured as a simple binary classification problem for each label. That is what we'll be doing - we'll train 6 differnet classifiers for the 6 labels (toxic, severe_toxic, obscene, threat, insult and identity_hate) given."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lets begin by training the model only to identify if a comment belong to 'obscene' category or not\n",
    "### Preparing the data - We will be using TF-IDF repesentation of the processed comments and split the data into train and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer()\n",
    "tfidf_comments = vectorizer.fit_transform(data[\"processed_comment_text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(159571, 185943)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_comments.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<159571x185943 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 4487267 stored elements in Compressed Sparse Row format>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(tfidf_comments) \n",
    "# TF-IDF return a sparse matrix represenation as it gives huge boost in terms of storage and computaion. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0.947052\n",
       "1    0.052948\n",
       "Name: obscene, dtype: float64"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[\"obscene\"].value_counts(normalize = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(127656, 185943)\n",
      "(127656,)\n",
      "(31915, 185943)\n",
      "(31915,)\n"
     ]
    }
   ],
   "source": [
    "# To train the model, the tf-idf represenation becomes the feature matrix and target column is label we're currently training for.\n",
    "# Since there is an imbalance in the target label, we'll use straified approach to split the data into train and test sets.\n",
    "train_x, test_x, train_y, test_y = train_test_split(tfidf_comments, data[\"obscene\"], test_size = 0.2, stratify = data[\"obscene\"], random_state = 20)\n",
    "\n",
    "print(train_x.shape)\n",
    "print(train_y.shape)\n",
    "print(test_x.shape)\n",
    "print(test_y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Naive-Bayes as our Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Naive-Bayes is classification technique based on applying Bayes' Theorem with strong independance assumptions betwen features. Naive-Bayes is a conditional probabilstic model, that gives a probability that a data point with a given set of features belong to a specific class. More formally, given a data point represented as a vector $x = (x_1, x_2, ..., x_n)$ (where the $n$ features are independant), Naive-Bayes assigns a probabilities $p(C_k|x)$ for each of the $K$ classes.\n",
    "\n",
    "Using Bayes' Theorem, the above probability value can be written as - \n",
    "$$p(C_k|x) = \\frac{p(C_k)p(x|C_k)}{p(x)}$$\n",
    "\n",
    "Here,\n",
    "* $p(C_k|x)$ is the posterior probability that the data point represented as $x$ belongs to class $C_k$\n",
    "* $p(C_k)$ is the prior probability for class $C_k$\n",
    "* $p(x|C_k)$ is the likelihood which is probility of a data point $x$ given a class $C_k$\n",
    "* $p(x)$ is the prior probability for data point $x$\n",
    "\n",
    "The major focus is only on the numerator of the above fraction, as the denominator does depend on the class label and is fixed for a given data point. \n",
    "\n",
    "Assuming *conditional independence* among all features in $x$ and using chain rule, $p(x|C_k)$ can be written as - \n",
    "$$p(x|C_k) = p(x_1|C_k)p(x_2|C_k)...p(x_n|C_k)$$\n",
    "\n",
    "Therefore,\n",
    "$$p(C_k|x) \\propto p(C_k)p(x_1|C_k)p(x_2|C_k)...p(x_n|C_k)$$\n",
    "\n",
    "In text classification, $x$ might be a BoW representation of the document. For example, if below is the BoW representation of a sentence - "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Sentence** | hate | you | love\n",
    ":--- | :---: | :---: | :---:\n",
    "**Hate You** | 1 | 1 | 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then the probability that this sentencce belongs to obscene category is - \n",
    "$$p(obscene=1 | hate=1,you=1,love=0) \\propto p(obscene = 1)p(hate=1|obscene=1)p(you=1|obscene=1)p(love=1|obscene=1)$$\n",
    "\n",
    "Similarly, the probability that the obove sentence does not belong to obscene category is - \n",
    "$$p(obscene=0 | hate=1,you=1,love=0) \\propto p(obscene = 0)p(hate=1|obscene=0)p(you=1|obscene=0)p(love=1|obscene=0)$$\n",
    "\n",
    "The sentence can be assigned the class(0 or 1) which has the maximum probability among the above two. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB()"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nb_model = MultinomialNB() # A variant of Naive Bayes which assumes the feature distribution to be multinomial. \n",
    "nb_model.fit(train_x, train_y) # Fits the model on train data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using AUC-ROC curve\n",
    "AUC(Area under the curve)-ROC(Receiver Operating Characteristics) is a perfomance meassurement metric for classification problem. ROC is the probability curve between TPR (True Postive Rate) and FPR (False Positive Rate) at different thresholds. Area under the ROC curve gives the AUC-ROC score. This score measures the separability of classes, or in other words it tells how much the model is capable of distingusihing between the classes. Higher the AUC-ROC score, better the model is.\n",
    "\n",
    "\n",
    "![ROC Curve](https://miro.medium.com/max/451/1*pk05QGzoWhCgRiiFbz-oKQ.png \"AUC-ROC\")\n",
    "\n",
    "\n",
    "$$TPR = \\frac{TP}{TP + FN}$$\n",
    "\n",
    "$$FPR = \\frac{FP}{TN + FP}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8702047170873283"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "roc_auc_score(test_y, nb_model.predict_proba(test_x)[:,1]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training for all 6 labels separately"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for : toxic\n",
      "ROC-AUC score on test set : 0.88\n",
      "Training for : severe_toxic\n",
      "ROC-AUC score on test set : 0.88\n",
      "Training for : obscene\n",
      "ROC-AUC score on test set : 0.87\n",
      "Training for : threat\n",
      "ROC-AUC score on test set : 0.74\n",
      "Training for : insult\n",
      "ROC-AUC score on test set : 0.87\n",
      "Training for : identity_hate\n",
      "ROC-AUC score on test set : 0.81\n",
      "\n",
      "\n",
      " Average AUC-ROC Score across all labels : 0.84\n"
     ]
    }
   ],
   "source": [
    "labels = [\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]\n",
    "all_scores = []\n",
    "for label in labels:\n",
    "    print(\"Training for : {}\".format(label))\n",
    "    train_x, test_x, train_y, test_y = train_test_split(tfidf_comments, data[label], test_size = 0.2, stratify = data[label], random_state = 20)\n",
    "    nb_model = MultinomialNB()\n",
    "    nb_model.fit(train_x, train_y)\n",
    "    score = roc_auc_score(test_y, nb_model.predict_proba(test_x)[:,1]) \n",
    "    print(\"ROC-AUC score on test set : {}\".format(round(score, 2)))\n",
    "    all_scores.append(score)\n",
    "\n",
    "print(\"\\n\\n Average AUC-ROC Score across all labels : {}\".format(round(np.mean(all_scores), 2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this module we mainly taked about the following items - \n",
    "1. Some of the techniques to represent textual documents as numeric vectors.\n",
    "2. Using Naive Bayes to get class probabilities for a text.\n",
    "3. Evaluating the multiheaded classification model using AUC-ROC.\n",
    "\n",
    "Further areas of exploration can be - \n",
    "1. Considering n-grams while creating the TF-IDF or BOW representation. \n",
    "2. Other techniques to represent a textual document numerically, like word2vec or embedding based techniques (eg - BERT).\n",
    "3. Other models for textual classification - Deep Neural Network based models (like LSTMs) etc.\n",
    "4. Parameter finetuning for the model used.\n",
    "5. Other techniques to evaluate the model performance - F1 score, AUC PR, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
